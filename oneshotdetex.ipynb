{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection.backbone_utils import mobilenet_backbone\n",
    "\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "nn = torch.nn\n",
    "import os\n",
    "import numpy as np\n",
    "from src.utils.audio_utils import compute_spectrogram, load_audio_file\n",
    "from src.utils import path_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneShotDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, detection_dir,transform_audio, device):\n",
    "        self.detection_dir = detection_dir\n",
    "        self.device = device\n",
    "        self.samplepath = os.path.join(self.detection_dir, 'samples')\n",
    "        self.targetpath = os.path.join(self.detection_dir, 'target')\n",
    "        self.indexes = list(map(lambda x: x.split('.')[0],os.listdir(self.samplepath)))\n",
    "        self.transform_audio = transform_audio\n",
    "        self.transform_image = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        self.spec_hight=120\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir((self.samplepath)))\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        index=self.indexes[idx]\n",
    "        data,fs = load_audio_file(os.path.join(self.samplepath,index +'.wav'))\n",
    "        targets = np.load(os.path.join(self.targetpath,index +'.npy'))\n",
    "        data = self.transform_audio(data)\n",
    "        boxes=torch.as_tensor([[start,0,end,data.shape[0]] for [_,start,end] in targets ], dtype=torch.float64)\n",
    "        labels=torch.as_tensor([target[0] for target in targets], dtype=torch.int64)\n",
    "        target_dict={'boxes':boxes,'labels':labels}\n",
    "        #target_dict=dict(filter(lambda x: x['boxes']!=[], target_dict.items()))\n",
    "        # targets=[{'boxes':torch.as_tensor([start,0,end,data.shape[0]], dtype=torch.float32),'labels':lbl} for [lbl,start,end]in targets]\n",
    "        x = self.transform_image(data)\n",
    "        return x,target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_audio(data):\n",
    "    _, _, specto = compute_spectrogram(data, 24000, nperseg=256, noverlap=256/2, scale=\"dB\")\n",
    "    # freq clip\n",
    "    specto = specto[:120, :]\n",
    "    return specto\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "detection_dataloader = torch.utils.data.DataLoader(OneShotDataset(detection_dir=path_utils.get_detection_data_path(),\n",
    "                                                    transform_audio=transform_audio, device=device),collate_fn=custom_collate,batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rcnn_pretrained_backbone(num_classes : int, anchor_sizes : tuple, aspect_ratios : tuple, parameters : dict = {}):\n",
    "    \"\"\"\n",
    "    Return a faster rcnn with a mobilenetv3 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes expected to return (background should be taken into account)\n",
    "        parameters (dict, optional): Dictionnary for the different following parameters. Defaults to {}.\n",
    "\n",
    "    Returns:\n",
    "        (torchvision.models.detection.faster_rcnn.FasterRCNN): Model implementation of pytorch\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pretrained_backbone=True\n",
    "    trainable_backbone_layers=6 # All backbone is trainable\n",
    "\n",
    "    backbone = mobilenet_backbone(\"mobilenet_v3_large\", pretrained_backbone, False, trainable_layers=trainable_backbone_layers)\n",
    "\n",
    "    model = torchvision.models.detection.faster_rcnn.FasterRCNN(backbone,\n",
    "                                                                num_classes,\n",
    "                                                                rpn_anchor_generator=AnchorGenerator(anchor_sizes, aspect_ratios),\n",
    "                                                                **parameters)\n",
    "\n",
    "    # Custom transform ie no transform (only postprocessing)\n",
    "    model.transform = GeneralizedRCNNTransform(min_size=489, max_size=2000, image_mean=[0, 0, 0], image_std=[1, 1, 1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rcnn_pretrained_backbone_train(num_classes : int, anchor_sizes : tuple, aspect_ratios : tuple, parameters : dict = {}):\n",
    "    \"\"\"\n",
    "    Return a jit compiled faster rcnn with a mobilenetv3 backbone.\n",
    "    \"\"\"\n",
    "    model = rcnn_pretrained_backbone(num_classes, anchor_sizes, aspect_ratios, parameters)\n",
    "    model._has_warned = True # Remove warning about \"RCNN always returns a (Losses, Detections) tuple in scripting\"\n",
    "    return torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_faster_rcnn(dict_losses : dict, training_rpn : bool, training_head : bool):\n",
    "    \"\"\" Reduce the dictionnary of losses\n",
    "\n",
    "    Args:\n",
    "        dict_losses (dict): Dictionnary of losses\n",
    "        training_rpn (bool): Bool to train the rpn\n",
    "        training_head (bool): Bool to train the head\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Loss\n",
    "    \"\"\"\n",
    "    # Dict(\"loss_classifier\", \"loss_box_reg\", \"loss_objectness\", \"loss_rpn_box_reg\")\n",
    "    loss = torch.zeros((), dtype=torch.float32, device=device)\n",
    "    if training_rpn:\n",
    "        loss += dict_losses[\"loss_objectness\"]\n",
    "        loss += dict_losses[\"loss_rpn_box_reg\"]\n",
    "    if training_head:\n",
    "        loss += dict_losses[\"loss_classifier\"]\n",
    "        loss += dict_losses[\"loss_box_reg\"]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_losses(dataloader, mean_loss):\n",
    "    mean_loss[\"loss_objectness\"] /= len(dataloader)\n",
    "    mean_loss[\"loss_rpn_box_reg\"] /= len(dataloader)\n",
    "    mean_loss[\"loss_classifier\"] /= len(dataloader)\n",
    "    mean_loss[\"loss_box_reg\"] /= len(dataloader)\n",
    "    return mean_loss\n",
    "\n",
    "def accumulate_losses(mean_loss, losses):\n",
    "    mean_loss[\"loss_objectness\"] += losses[\"loss_objectness\"].item()\n",
    "    mean_loss[\"loss_rpn_box_reg\"] += losses[\"loss_rpn_box_reg\"].item()\n",
    "    mean_loss[\"loss_classifier\"] += losses[\"loss_classifier\"].item()\n",
    "    mean_loss[\"loss_box_reg\"] += losses[\"loss_box_reg\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rcnn_pretrained_backbone_train(num_classes=2,anchor_sizes=((32, 64),),\n",
    "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "sgd_optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(sgd_optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "loss_fn_frrcnn=lambda x:loss_faster_rcnn(x,True,True)\n",
    "                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, macro_batch=1):\n",
    "    # Initialize training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    mean_loss = {\"loss_objectness\": 0., \"loss_rpn_box_reg\": 0., \"loss_classifier\": 0., \"loss_box_reg\": 0.}\n",
    "    # Iterate over the dataset\n",
    "    for batch, (X, targets) in enumerate(dataloader):\n",
    "        # Work with the GPU if available\n",
    "        X = list(x.to(device) for x in X)\n",
    "        targets = list({k: v.to(device) for k, v in t.items()} for t in targets)\n",
    "        # Compute prediction error\n",
    "        losses = model(X, targets)[0]\n",
    "        accumulate_losses(mean_loss, losses)\n",
    "        loss = loss_fn(losses)\n",
    "        print(loss)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "      \n",
    "        if (batch+1) % macro_batch == 0 or batch == len(dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        # Print metrics\n",
    "        if batch % 30 == 0 or batch == len(dataloader) - 1:\n",
    "            loss_value, current = loss.item(), (batch+1) * len(X)\n",
    "            print(loss_value)\n",
    "    return average_losses(dataloader, mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('boxes', tensor([[12547.,     0., 33913.,   120.],\n",
      "        [ 2934.,     0., 34988.,   120.],\n",
      "        [10122.,     0., 34808.,   120.],\n",
      "        [ 9361.,     0., 39144.,   120.],\n",
      "        [17833.,     0., 44625.,   120.],\n",
      "        [12822.,     0., 42448.,   120.],\n",
      "        [  226.,     0., 47950.,   120.],\n",
      "        [ 1628.,     0., 43487.,   120.],\n",
      "        [  279.,     0., 44493.,   120.],\n",
      "        [ 3850.,     0., 43313.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0]))])\n",
      "dict_items([('boxes', tensor([[ 1287.,     0., 33466.,   120.],\n",
      "        [14715.,     0., 38766.,   120.],\n",
      "        [19001.,     0., 39948.,   120.],\n",
      "        [ 2932.,     0., 43095.,   120.],\n",
      "        [  297.,     0., 42730.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 1, 0, 0]))])\n",
      "tensor(2027.9116, grad_fn=<AddBackward0>)\n",
      "2027.91162109375\n",
      "dict_items([('boxes', tensor([[19043.,     0., 43425.,   120.],\n",
      "        [18684.,     0., 39894.,   120.],\n",
      "        [ 4423.,     0., 33920.,   120.],\n",
      "        [20563.,     0., 44426.,   120.],\n",
      "        [22363.,     0., 39071.,   120.],\n",
      "        [ 3072.,     0., 27335.,   120.],\n",
      "        [10383.,     0., 30368.,   120.],\n",
      "        [ 3097.,     0., 41969.,   120.],\n",
      "        [13058.,     0., 38143.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]))])\n",
      "dict_items([('boxes', tensor([[ 8585.,     0., 38298.,   120.],\n",
      "        [14024.,     0., 40097.,   120.],\n",
      "        [ 3151.,     0., 45716.,   120.],\n",
      "        [ 1725.,     0., 42337.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 0, 0]))])\n",
      "tensor(2710.8645, grad_fn=<AddBackward0>)\n",
      "dict_items([('boxes', tensor([[ 9791.,     0., 40431.,   120.],\n",
      "        [10133.,     0., 47647.,   120.],\n",
      "        [ 5871.,     0., 45054.,   120.],\n",
      "        [ 7145.,     0., 47400.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 0, 0]))])\n",
      "dict_items([('boxes', tensor([[ 9165.,     0., 46136.,   120.],\n",
      "        [ 8184.,     0., 41184.,   120.],\n",
      "        [ 9135.,     0., 34486.,   120.],\n",
      "        [17507.,     0., 40878.,   120.],\n",
      "        [16582.,     0., 47144.,   120.],\n",
      "        [ 1189.,     0., 32127.,   120.],\n",
      "        [ 3954.,     0., 47933.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 1, 1, 1, 1, 0]))])\n",
      "tensor(139215.0625, grad_fn=<AddBackward0>)\n",
      "dict_items([('boxes', tensor([[ 4048.,     0., 28036.,   120.],\n",
      "        [ 1526.,     0., 21722.,   120.],\n",
      "        [17935.,     0., 43483.,   120.],\n",
      "        [  473.,     0., 28371.,   120.],\n",
      "        [ 9684.,     0., 43350.,   120.],\n",
      "        [  128.,     0., 40654.,   120.],\n",
      "        [ 5215.,     0., 46454.,   120.],\n",
      "        [ 2565.,     0., 45367.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 1, 1, 1, 0, 0, 0]))])\n",
      "dict_items([('boxes', tensor([[16338.,     0., 41829.,   120.],\n",
      "        [ 5431.,     0., 40283.,   120.],\n",
      "        [ 1602.,     0., 19403.,   120.],\n",
      "        [13080.,     0., 32547.,   120.],\n",
      "        [  999.,     0., 35704.,   120.],\n",
      "        [ 2624.,     0., 33417.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 1, 1, 1, 1]))])\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "dict_items([('boxes', tensor([[ 4394.,     0., 34283.,   120.],\n",
      "        [ 7031.,     0., 35794.,   120.],\n",
      "        [ 5400.,     0., 46043.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 0]))])\n",
      "dict_items([('boxes', tensor([[24740.,     0., 43771.,   120.],\n",
      "        [10546.,     0., 32936.,   120.],\n",
      "        [14424.,     0., 42121.,   120.],\n",
      "        [16905.,     0., 36179.,   120.],\n",
      "        [ 1774.,     0., 26928.,   120.]], dtype=torch.float64)), ('labels', tensor([1, 1, 1, 1, 1]))])\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000011?line=0'>1</a>\u001b[0m train_loop(dataloader\u001b[39m=\u001b[39;49mdetection_dataloader, model\u001b[39m=\u001b[39;49mmodel, loss_fn\u001b[39m=\u001b[39;49mloss_fn_frrcnn, optimizer\u001b[39m=\u001b[39;49msgd_optimizer, scheduler\u001b[39m=\u001b[39;49mlr_scheduler, macro_batch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb Cell 13'\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, scheduler, macro_batch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000010?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000010?line=16'>17</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000010?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000010?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m (batch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m macro_batch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m batch \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(dataloader) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000010?line=20'>21</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/P3_BBF/audio_env/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/Desktop/P3_BBF/audio_env/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(dataloader=detection_dataloader, model=model, loss_fn=loss_fn_frrcnn, optimizer=sgd_optimizer, scheduler=lr_scheduler, macro_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('audio_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2db47ff5d915a88877d09c74664ee98c88dbd61c7ef890d46532488c99d9b1a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
