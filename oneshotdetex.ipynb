{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection.backbone_utils import mobilenet_backbone\n",
    "\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "nn = torch.nn\n",
    "import os\n",
    "import numpy as np\n",
    "from src.utils.audio_utils import compute_mel_spectrogram, compute_spectrogram, load_audio_file\n",
    "from src.utils import path_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneShotDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, detection_dir,transform_audio, device):\n",
    "        self.detection_dir = detection_dir\n",
    "        self.device = device\n",
    "        self.samplepath = os.path.join(self.detection_dir, 'samples')\n",
    "        self.targetpath = os.path.join(self.detection_dir, 'target')\n",
    "        self.indexes = list(map(lambda x: x.split('.')[0],os.listdir(self.samplepath)))\n",
    "        self.transform_audio = transform_audio\n",
    "        self.transform_image = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        self.spec_hight=120\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir((self.samplepath)))\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        index=self.indexes[idx]\n",
    "        data,fs = load_audio_file(os.path.join(self.samplepath,index +'.wav'))\n",
    "        target = np.load(os.path.join(self.targetpath,index +'.npy'))\n",
    "        bbox=[[cls,xs,0,xe,self.spec_hight] for [cls,xs,xe]in target]\n",
    "        print(bbox)\n",
    "        #  bbox=[[target[0],target[1],0,target[2],self.spec_hight] for  target in targets]\n",
    "\n",
    "        x = self.transform_audio(data)\n",
    "        x = self.transform_image(x)\n",
    "        return x, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_audio(data):\n",
    "    _, _, specto = compute_spectrogram(data, 24000, nperseg=256, noverlap=256/2, scale=\"dB\")\n",
    "    # freq clip\n",
    "    specto = specto[:120, :]\n",
    "    return specto\n",
    "    \n",
    "detection_dataloader = torch.utils.data.DataLoader(OneShotDataset(detection_dir=path_utils.get_detection_data_path(),\n",
    "                                                    transform_audio=transform_audio, device=device),batch_size=1)\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[0, 1638, 0, 44090, 120]]\n",
      "[[0, 4553, 0, 43908, 120]]\n",
      "[]\n",
      "[[1, 17817, 0, 36119, 120], [1, 19995, 0, 43395, 120]]\n",
      "[]\n",
      "[[1, 7540, 0, 37059, 120], [1, 9072, 0, 38563, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 1985, 0, 47987, 120]]\n",
      "[]\n",
      "[[0, 1585, 0, 43084, 120], [0, 3818, 0, 47034, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 2001, 0, 46826, 120]]\n",
      "[]\n",
      "[[1, 13302, 0, 44797, 120]]\n",
      "[[0, 1717, 0, 46597, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 352, 0, 47957, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 6425, 0, 47708, 120]]\n",
      "[[0, 365, 0, 47287, 120]]\n",
      "[]\n",
      "[[1, 23730, 0, 46873, 120]]\n",
      "[]\n",
      "[[1, 7341, 0, 42266, 120]]\n",
      "[]\n",
      "[]\n",
      "[[1, 9986, 0, 29587, 120], [1, 7408, 0, 24942, 120]]\n",
      "[]\n",
      "[[0, 3559, 0, 44701, 120]]\n",
      "[[0, 8433, 0, 46906, 120]]\n",
      "[[1, 17810, 0, 40052, 120], [0, 487, 0, 46093, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[0, 2169, 0, 47021, 120]]\n",
      "[[1, 6376, 0, 32335, 120], [0, 7628, 0, 47490, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[1, 8055, 0, 41819, 120]]\n",
      "[]\n",
      "[[1, 8014, 0, 38541, 120]]\n",
      "[[1, 13313, 0, 45948, 120]]\n",
      "[[1, 10052, 0, 42880, 120]]\n",
      "[[1, 8887, 0, 35579, 120], [0, 5421, 0, 45365, 120]]\n",
      "[[0, 2744, 0, 45143, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 1624, 0, 42653, 120]]\n",
      "[]\n",
      "[[1, 6700, 0, 24248, 120], [0, 2689, 0, 46159, 120], [0, 3495, 0, 42721, 120], [0, 2257, 0, 46225, 120]]\n",
      "[]\n",
      "[[1, 10644, 0, 38265, 120]]\n",
      "[[1, 3839, 0, 36625, 120]]\n",
      "[[1, 12687, 0, 44688, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[1, 6602, 0, 26789, 120], [1, 9871, 0, 28446, 120]]\n",
      "[]\n",
      "[]\n",
      "[[1, 2047, 0, 30296, 120]]\n",
      "[[1, 15204, 0, 39515, 120], [0, 571, 0, 42468, 120]]\n",
      "[[0, 1490, 0, 41301, 120], [0, 3261, 0, 42893, 120]]\n",
      "[]\n",
      "[[0, 206, 0, 47913, 120]]\n",
      "[]\n",
      "[]\n",
      "[[1, 6679, 0, 38264, 120]]\n",
      "[]\n",
      "[[1, 16387, 0, 38525, 120]]\n",
      "[]\n",
      "[]\n",
      "[[1, 24679, 0, 44785, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[1, 28675, 0, 47332, 120], [0, 3112, 0, 47230, 120]]\n",
      "[[1, 2398, 0, 31064, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 3534, 0, 43922, 120]]\n",
      "[[1, 18394, 0, 44637, 120]]\n",
      "[[0, 184, 0, 45013, 120]]\n",
      "[[0, 537, 0, 41761, 120]]\n",
      "[[0, 1188, 0, 46644, 120]]\n",
      "[[0, 4177, 0, 47536, 120]]\n",
      "[]\n",
      "[]\n",
      "[[1, 8755, 0, 36881, 120]]\n",
      "[[0, 347, 0, 43694, 120]]\n",
      "[[0, 2039, 0, 47641, 120]]\n",
      "[[1, 9670, 0, 46599, 120], [1, 9561, 0, 32896, 120]]\n",
      "[[0, 1817, 0, 46352, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[0, 629, 0, 40631, 120]]\n",
      "[]\n",
      "[]\n",
      "[[1, 3553, 0, 27308, 120], [1, 1829, 0, 39664, 120]]\n",
      "[[0, 484, 0, 46563, 120]]\n",
      "[]\n",
      "[[1, 231, 0, 41175, 120]]\n",
      "[]\n",
      "[[1, 25420, 0, 47147, 120]]\n",
      "[[0, 3353, 0, 46419, 120]]\n",
      "[[1, 3284, 0, 31006, 120], [0, 6858, 0, 47886, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 526, 0, 43227, 120]]\n",
      "[[1, 9303, 0, 31562, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 3438, 0, 44416, 120]]\n",
      "[[1, 18260, 0, 39891, 120], [0, 491, 0, 47777, 120]]\n",
      "[[0, 126, 0, 47591, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[0, 742, 0, 42105, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[1, 13560, 0, 43136, 120]]\n",
      "[]\n",
      "[[1, 3105, 0, 40593, 120]]\n",
      "[[0, 3611, 0, 47431, 120]]\n",
      "[[1, 5419, 0, 34880, 120]]\n",
      "[[0, 180, 0, 47789, 120]]\n",
      "[]\n",
      "[]\n",
      "[[0, 378, 0, 41736, 120]]\n",
      "[]\n",
      "[[0, 2609, 0, 46047, 120], [0, 54, 0, 47891, 120]]\n",
      "[[0, 43, 0, 47991, 120]]\n",
      "[]\n",
      "[[0, 1977, 0, 43959, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[1, 20467, 0, 45632, 120]]\n",
      "[]\n",
      "[]\n",
      "[[1, 12579, 0, 36107, 120]]\n",
      "[]\n",
      "[[1, 17256, 0, 46767, 120]]\n",
      "[]\n",
      "[]\n",
      "[[1, 17522, 0, 44527, 120]]\n",
      "[[1, 23508, 0, 43301, 120], [0, 4016, 0, 47032, 120]]\n",
      "[[0, 1392, 0, 40047, 120]]\n",
      "[[1, 6753, 0, 29755, 120]]\n",
      "[[1, 2697, 0, 36333, 120]]\n",
      "[[1, 1228, 0, 21264, 120], [1, 22690, 0, 45981, 120]]\n",
      "[]\n",
      "[[0, 334, 0, 40378, 120]]\n",
      "[[1, 15500, 0, 34165, 120]]\n",
      "[[1, 12196, 0, 41328, 120], [1, 11416, 0, 38068, 120]]\n",
      "[[0, 1634, 0, 43344, 120]]\n",
      "[[0, 3147, 0, 44312, 120]]\n",
      "[[1, 6536, 0, 34776, 120]]\n",
      "[]\n",
      "[[1, 15648, 0, 37244, 120], [0, 732, 0, 47623, 120]]\n",
      "[[0, 351, 0, 46999, 120]]\n",
      "[]\n",
      "[[1, 30724, 0, 47273, 120]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for (x,y)in detection_dataloader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rcnn_pretrained_backbone(num_classes : int, anchor_sizes : tuple, aspect_ratios : tuple, parameters : dict = {}):\n",
    "    \"\"\"\n",
    "    Return a faster rcnn with a mobilenetv3 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes expected to return (background should be taken into account)\n",
    "        parameters (dict, optional): Dictionnary for the different following parameters. Defaults to {}.\n",
    "\n",
    "    Returns:\n",
    "        (torchvision.models.detection.faster_rcnn.FasterRCNN): Model implementation of pytorch\n",
    "\n",
    "    >>> training\n",
    "    # RPN\n",
    "    rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
    "    rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
    "    rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
    "        considered as positive during training of the RPN.\n",
    "    rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
    "        considered as negative during training of the RPN.\n",
    "    rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
    "        for computing the loss\n",
    "    rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
    "        of the RPN\n",
    "\n",
    "    # Head\n",
    "    box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
    "        considered as positive during training of the classification head\n",
    "    box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
    "        considered as negative during training of the classification head\n",
    "    box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
    "        classification head\n",
    "    box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
    "        of the classification head\n",
    "\n",
    "\n",
    "\n",
    "    >>> inference\n",
    "    # RPN\n",
    "    rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
    "    rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
    "    rpn_score_thresh (float): during inference, only return proposals with a classification score\n",
    "        greater than rpn_score_thresh\n",
    "    rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
    "\n",
    "    # Head\n",
    "    box_score_thresh (float): during inference, only return proposals with a classification score\n",
    "        greater than box_score_thresh\n",
    "\n",
    "    box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
    "    box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pretrained_backbone=True\n",
    "    trainable_backbone_layers=6 # All backbone is trainable\n",
    "\n",
    "    backbone = mobilenet_backbone(\"mobilenet_v3_large\", pretrained_backbone, True, trainable_layers=trainable_backbone_layers)\n",
    "\n",
    "    model = torchvision.models.detection.faster_rcnn.FasterRCNN(backbone,\n",
    "                                                                num_classes,\n",
    "                                                                rpn_anchor_generator=AnchorGenerator(anchor_sizes, aspect_ratios),\n",
    "                                                                **parameters)\n",
    "\n",
    "    # Custum transform ie no transform (only postprocessing)\n",
    "    model.transform = GeneralizedRCNNTransform(min_size=489, max_size=2000, image_mean=[0, 0, 0], image_std=[1, 1, 1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rcnn_pretrained_backbone_train(num_classes : int, anchor_sizes : tuple, aspect_ratios : tuple, parameters : dict = {}):\n",
    "    \"\"\"\n",
    "    Return a jit compiled faster rcnn with a mobilenetv3 backbone.\n",
    "    \"\"\"\n",
    "    model = rcnn_pretrained_backbone(num_classes, anchor_sizes, aspect_ratios, parameters)\n",
    "    model._has_warned = True # Remove warning about \"RCNN always returns a (Losses, Detections) tuple in scripting\"\n",
    "    return torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_faster_rcnn(dict_losses : dict, training_rpn : bool, training_head : bool):\n",
    "    \"\"\" Reduce the dictionnary of losses\n",
    "\n",
    "    Args:\n",
    "        dict_losses (dict): Dictionnary of losses\n",
    "        training_rpn (bool): Bool to train the rpn\n",
    "        training_head (bool): Bool to train the head\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Loss\n",
    "    \"\"\"\n",
    "    # Dict(\"loss_classifier\", \"loss_box_reg\", \"loss_objectness\", \"loss_rpn_box_reg\")\n",
    "    loss = torch.zeros((), dtype=torch.float32, device=device)\n",
    "    if training_rpn:\n",
    "        loss += dict_losses[\"loss_objectness\"]\n",
    "        loss += dict_losses[\"loss_rpn_box_reg\"]\n",
    "    if training_head:\n",
    "        loss += dict_losses[\"loss_classifier\"]\n",
    "        loss += dict_losses[\"loss_box_reg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_losses(dataloader, mean_loss):\n",
    "    mean_loss[\"loss_objectness\"] /= len(dataloader)\n",
    "    mean_loss[\"loss_rpn_box_reg\"] /= len(dataloader)\n",
    "    mean_loss[\"loss_classifier\"] /= len(dataloader)\n",
    "    mean_loss[\"loss_box_reg\"] /= len(dataloader)\n",
    "    return mean_loss\n",
    "\n",
    "def accumulate_losses(mean_loss, losses):\n",
    "    mean_loss[\"loss_objectness\"] += losses[\"loss_objectness\"].item()\n",
    "    mean_loss[\"loss_rpn_box_reg\"] += losses[\"loss_rpn_box_reg\"].item()\n",
    "    mean_loss[\"loss_classifier\"] += losses[\"loss_classifier\"].item()\n",
    "    mean_loss[\"loss_box_reg\"] += losses[\"loss_box_reg\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_audio(data):\n",
    "    _, _, specto = compute_spectrogram(data, 24000, nperseg=256, noverlap=256/2, scale=\"dB\")\n",
    "    # freq clip\n",
    "    specto = specto[:120, :]\n",
    "    return specto\n",
    "    \n",
    "detection_dataloader = torch.utils.data.DataLoader(OneShotDataset(detection_dir=path_utils.get_detection_data_path(),\n",
    "                                                    transform_audio=transform_audio, device=device),batch_size=1)\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rcnn_pretrained_backbone_train(num_classes=4,anchor_sizes=[32,64,128],aspect_ratios=[0.5,1,2])\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "sgd_optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(sgd_optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "                                        \n",
    "loss_fn_frrcnn=lambda x:loss_faster_rcnn(x,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, macro_batch=1):\n",
    "    # Initialize training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    mean_loss = {\"loss_objectness\": 0., \"loss_rpn_box_reg\": 0., \"loss_classifier\": 0., \"loss_box_reg\": 0.}\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for batch, (X, targets) in enumerate(dataloader):\n",
    "        # Work with the GPU if available\n",
    "        X = list(x.to(device) for x in X)\n",
    "        #targets = list({k: v.to(device) for k, v in t.item()} for t in targets)\n",
    "\n",
    "        # Compute prediction error\n",
    "        losses, detections = model(X, targets)\n",
    "        accumulate_losses(mean_loss, losses)\n",
    "        loss = loss_fn(losses)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        if (batch+1) % macro_batch == 0 or batch == len(dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Print metrics\n",
    "        if batch % 30 == 0 or batch == len(dataloader) - 1:\n",
    "            loss_value, current = loss.item(), (batch+1) * len(X)\n",
    "        print(loss_value)\n",
    "    return average_losses(dataloader, mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/Users/paulbp_dty/Desktop/P3_BBF/audio_env/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 65, in forward\n            assert targets is not None\n            for target in targets:\n                boxes = target[\"boxes\"]\n                        ~~~~~~~~~~~~~~ <--- HERE\n                if isinstance(boxes, torch.Tensor):\n                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\nRuntimeError: KeyError: boxes\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000011?line=0'>1</a>\u001b[0m train_loop(dataloader\u001b[39m=\u001b[39;49mdetection_dataloader, model\u001b[39m=\u001b[39;49mmodel, loss_fn\u001b[39m=\u001b[39;49mloss_fn_frrcnn, optimizer\u001b[39m=\u001b[39;49msgd_optimizer, scheduler\u001b[39m=\u001b[39;49mlr_scheduler, macro_batch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb Cell 10'\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, scheduler, macro_batch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000014?line=10'>11</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(x\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000014?line=11'>12</a>\u001b[0m \u001b[39m#targets = list({k: v.to(device) for k, v in t.item()} for t in targets)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000014?line=12'>13</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000014?line=13'>14</a>\u001b[0m \u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000014?line=14'>15</a>\u001b[0m losses, detections \u001b[39m=\u001b[39m model(X, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000014?line=15'>16</a>\u001b[0m accumulate_losses(mean_loss, losses)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paulbp_dty/Desktop/P3_BBF/masked-owl-detection/oneshotdetex.ipynb#ch0000014?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(losses)\n",
      "File \u001b[0;32m~/Desktop/P3_BBF/audio_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/Users/paulbp_dty/Desktop/P3_BBF/audio_env/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 65, in forward\n            assert targets is not None\n            for target in targets:\n                boxes = target[\"boxes\"]\n                        ~~~~~~~~~~~~~~ <--- HERE\n                if isinstance(boxes, torch.Tensor):\n                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\nRuntimeError: KeyError: boxes\n"
     ]
    }
   ],
   "source": [
    "train_loop(dataloader=detection_dataloader, model=model, loss_fn=loss_fn_frrcnn, optimizer=sgd_optimizer, scheduler=lr_scheduler, macro_batch=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('audio_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2db47ff5d915a88877d09c74664ee98c88dbd61c7ef890d46532488c99d9b1a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
